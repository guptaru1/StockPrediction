{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERR_-ZIJvio7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "try:\n",
        "    import yahoo_fin.stock_info as si\n",
        "except ModuleNotFoundError:\n",
        "    if 'google.colab' in str(get_ipython()):\n",
        "        !pip install yahoo_fin\n",
        "    import yahoo_fin.stock_info as si\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "mutation_prob = 0.2\n",
        "crossover_prob = 0.8\n",
        "initial_population = 100\n",
        "generations = 150\n",
        "rsi_threshold = 30\n",
        "ema_weight = 0.8\n",
        "\n",
        "\n",
        "def pre_process(stock_data):\n",
        "  data = stock_data[[\"close\"]]\n",
        "  data = data.rename(columns= {'close': 'actualclose'})\n",
        "  #this is so that we can see which is higer or above in the stock\n",
        "  #data[\"Target\"] = stock_data.rolling(2).apply(lambda x: x.iloc[1] > x.iloc[0])[\"Actual_Close\"]\n",
        "  data['Target'] = (data['actualclose'] > data['actualclose'].shift(1)).astype(int)\n",
        "  stock_data_copy = stock_data.copy()\n",
        "  stock_data_shifted = stock_data_copy.shift(1)\n",
        "\n",
        "  #combine training data\n",
        "  predictors = [\"close\", \"volume\", \"open\", \"high\", \"low\"]\n",
        "  data = data.join(stock_data_shifted[predictors]).iloc[1:]\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "#calculate simple moving average using %d which is usually a 3 day period\n",
        "def calculate_stochastic_oscillator(data, k_period=14, d_period=3):\n",
        "    # Calculate %K\n",
        "    data['Lowest Low'] = data['low'].rolling(window=k_period).min()\n",
        "    data['Highest High'] = data['high'].rolling(window=k_period).max()\n",
        "    data['%K'] = ((data['close'] - data['Lowest Low']) / (data['Highest High'] - data['Lowest Low'])) * 100\n",
        "    # Calculate %D (3-period SMA of %K)\n",
        "    data['%D'] = data['%K'].rolling(window=d_period).mean()\n",
        "    # Drop auxiliary columns\n",
        "    data.drop(['Lowest Low', 'Highest High'], axis=1, inplace=True)\n",
        "    return data[['%K', '%D']]\n",
        "\n",
        "\n",
        "def calculate_features(data):\n",
        "    close_prices = data['close']\n",
        "    #rsi\n",
        "    #rsi = talib.RSI(close_prices, timeperiod=14)\n",
        "    #calcualte ema\n",
        "    #sma for a 200 period\n",
        "    data[\"200day\"] = data[\"close\"].rolling(200).mean()\n",
        "    #sma for a 50 day period\n",
        "    data[\"50day\"] = data[\"close\"].rolling(50).mean()\n",
        "    print(\"new features\")\n",
        "    print(data.head(5))\n",
        "\n",
        "\n",
        "\n",
        "def sharpe_ratio(weights, data):\n",
        "  data_returns = np.log(data) - np.log(data.shift(1))\n",
        "  data_returns = data_returns.dropna()\n",
        "\n",
        "  portfolio_returns = np.dot(data_returns, weights)\n",
        "  portfolio_mean = np.mean(portfolio_returns)\n",
        "  portfolio_std = np.std(portfolio_returns)\n",
        "  sharpe_ratio = (portfolio_mean / portfolio_std) * np.sqrt(252)\n",
        "  return sharpe_ratio\n",
        "\n",
        "\n",
        "# Scrape stock data from Yahoo Finance\n",
        "\n",
        "stock_data = si.get_data(\"AAPL\")\n",
        "'''\n",
        "stock_data = si.download('AAPL',\n",
        "                      start='2010-01-01',\n",
        "                      end='2021-06-12',\n",
        "                      progress=False,\n",
        ")\n",
        "'''\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def calculate_stochastic_oscillator(data, k_period=14, d_period=3):\n",
        "    # Calculate %K\n",
        "    data['Lowest Low'] = data['low'].rolling(window=k_period).min()\n",
        "    data['Highest High'] = data['high'].rolling(window=k_period).max()\n",
        "    data['%K'] = ((data['close'] - data['Lowest Low']) / (data['Highest High'] - data['Lowest Low'])) * 100\n",
        "    # Calculate %D (3-period SMA of %K)\n",
        "    data['%D'] = data['%K'].rolling(window=d_period).mean()\n",
        "    # Drop auxiliary columns\n",
        "    data.drop(['Lowest Low', 'Highest High'], axis=1, inplace=True)\n",
        "    return data[['%K', '%D']]\n",
        "\n",
        "\n",
        "def esa(data):\n",
        "    close_prices = data['close']\n",
        "    #rsi\n",
        "    #rsi = talib.RSI(close_prices, timeperiod=14)\n",
        "    #calcualte ema\n",
        "    #sma for a 200 period\n",
        "    data[\"200day\"] = data[\"close\"].rolling(200).mean()\n",
        "    #sma for a 50 day period\n",
        "    data[\"50day\"] = data[\"close\"].rolling(50).mean()\n",
        "    data[\"26day\"] = data[\"close\"].rolling(26).mean()\n",
        "    data[\"12day\"] = data[\"close\"].rolling(12).mean()\n",
        "    data['dif'] = data['12day'] - data['26day']\n",
        "    columns_to_drop = [\"26day\", \"12day\"]\n",
        "\n",
        "    # Calculate the rolling mean (middle band)\n",
        "    # Drop the specified columns\n",
        "    data.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "def boilinger_band(data, window=20):\n",
        "    # Calculate the rolling mean and standard deviation\n",
        "    rolling_mean = data['close'].rolling(window=window).mean()\n",
        "    rolling_std = data['close'].rolling(window=window).std()\n",
        "\n",
        "    # Calculate upper and lower bands\n",
        "    data[\"upper_band\"] = rolling_mean + (rolling_std * num_std)\n",
        "    data[\"lower_band\"] = rolling_mean - (rolling_std * num_std)\n",
        "\n",
        "\n",
        "def calculate_ichimoku_cloud(data, tenkan_period=9, kijun_period=26, senkou_span_b_period=52, displacement=26):\n",
        "    # Calculate Tenkan-sen\n",
        "    data['Tenkan-sen'] = (data['high'].rolling(window=tenkan_period).max() + data['low'].rolling(window=tenkan_period).min()) / 2\n",
        "\n",
        "    # Calculate Kijun-sen\n",
        "    data['Kijun-sen'] = (data['high'].rolling(window=kijun_period).max() + data['low'].rolling(window=kijun_period).min()) / 2\n",
        "\n",
        "    # Calculate Senkou Span A\n",
        "    data['Senkou Span A'] = ((data['Tenkan-sen'] + data['Kijun-sen']) / 2).shift(displacement)\n",
        "\n",
        "    # Calculate Senkou Span B\n",
        "    data['Senkou Span B'] = ((data['high'].rolling(window=senkou_span_b_period).max() + data['low'].rolling(window=senkou_span_b_period).min()) / 2).shift(displacement)\n",
        "\n",
        "\n",
        "def cal_fsl_udl(data):\n",
        "  data['price_change'] = data['close'].diff()\n",
        "\n",
        "  # Calculate Upside and Downside Volumes\n",
        "  data['upside_volume'] = data['volume'].where(data['price_change'] > 0, 0)\n",
        "  data['downside_volume'] = data['volume'].where(data['price_change'] < 0, 0)\n",
        "\n",
        "  # Calculate Upside Downside Line (UDL)\n",
        "  data['udl'] = data['upside_volume'].cumsum() - data['downside_volume'].cumsum()\n",
        "\n",
        "  # Calculate Force Index (FSL)\n",
        "  data['fsl'] = data['price_change'] * data['volume']\n",
        "\n",
        "  columns_to_drop = [\"price_change\" , \"upside_volume\", \"downside_volume\"]\n",
        "  data.drop(columns = columns_to_drop, inplace=True)\n",
        "\n",
        "\n",
        "def calculate_rsi(data, period=14):\n",
        "    # Calculate price changes\n",
        "    delta = data['close'].diff(1)\n",
        "\n",
        "    # Calculate gain (positive price changes) and loss (negative price changes)\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "\n",
        "    # Calculate average gain and average loss over the specified period\n",
        "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
        "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
        "\n",
        "    # Calculate relative strength (RS)\n",
        "    rs = avg_gain / avg_loss\n",
        "\n",
        "    # Calculate RSI\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "    data[\"rsi\"] = rsi\n",
        "\n",
        "def accumulate_distribution(df):\n",
        "    df['A/D'] = (df['close'] - df['low'] - (df['high'] - df['close'])) / (df['high'] - df['low']) * df['volume']\n",
        "    df['A/D'] = df['A/D'].cumsum()  # Cumulative sum to get the Accumulation/Distribution line\n",
        "\n",
        "\n",
        "def CCI(df):\n",
        "  # Calculate Typical Price (TP)\n",
        "  df['TP'] = (df['high'] + df['low'] + df['close']) / 3\n",
        "  # Calculate 10-period Simple Moving Average (SMA) of Typical Prices\n",
        "  df['SMA10'] = df['TP'].rolling(window=10).mean()\n",
        "  # Calculate Mean Deviation (MD)\n",
        "  df['MD'] = df['TP'] - df['SMA10']\n",
        "  df['MD'] = df['MD'].abs().rolling(window=10).sum()\n",
        "  # Calculate Commodity Channel Index (CCI)\n",
        "  factor = 0.015\n",
        "  df['CCI'] = (df['TP'] - df['SMA10']) / (factor * df['MD'])\n",
        "  # Drop intermediate columns (TP, SMA10, MD)\n",
        "  df.drop(['TP', 'SMA10', 'MD'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "def sharpe_ratio(weights, data):\n",
        "  data_returns = np.log(data) - np.log(data.shift(1))\n",
        "  data_returns = data_returns.dropna()\n",
        "\n",
        "  portfolio_returns = np.dot(data_returns, weights)\n",
        "  portfolio_mean = np.mean(portfolio_returns)\n",
        "  portfolio_std = np.std(portfolio_returns)\n",
        "  sharpe_ratio = (portfolio_mean / portfolio_std) * np.sqrt(252)\n",
        "  return sharpe_ratio\n",
        "\n",
        "\n",
        "def calculate_features(processed_data, feature_functions):\n",
        "  for func in feature_functions:\n",
        "        func(processed_data)\n",
        "  return processed_data\n",
        "\n",
        "def calculate_adx(data, period=14):\n",
        "    # Calculate True Range (TR)\n",
        "    data['High-Low'] = data['high'] - data['low']\n",
        "    data['High-PrevClose'] = abs(data['high'] - data['close'].shift(1))\n",
        "    data['Low-PrevClose'] = abs(data['low'] - data['close'].shift(1))\n",
        "    data['TR'] = data[['High-Low', 'High-PrevClose', 'Low-PrevClose']].max(axis=1)\n",
        "\n",
        "    # Calculate +DM and -DM\n",
        "    data['UpMove'] = data['high'].diff()\n",
        "    data['DownMove'] = -data['low'].diff()\n",
        "    data['+DM'] = data['UpMove'].where((data['UpMove'] > data['DownMove']) & (data['UpMove'] > 0), 0)\n",
        "    data['-DM'] = data['DownMove'].where((data['DownMove'] > data['UpMove']) & (data['DownMove'] > 0), 0)\n",
        "\n",
        "    # Smoothed TR, +DM, and -DM\n",
        "    data['ATR'] = data['TR'].ewm(span=period, adjust=False).mean()\n",
        "    data['+DM_Smoothed'] = data['+DM'].ewm(span=period, adjust=False).mean()\n",
        "    data['-DM_Smoothed'] = data['-DM'].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "    # Calculate +DI and -DI\n",
        "    data['+DI'] = (data['+DM_Smoothed'] / data['ATR']) * 100\n",
        "    data['-DI'] = (data['-DM_Smoothed'] / data['ATR']) * 100\n",
        "\n",
        "    # Calculate DX\n",
        "    data['DX'] = (abs(data['+DI'] - data['-DI']) / (data['+DI'] + data['-DI'])) * 100\n",
        "\n",
        "    # Calculate ADX\n",
        "    data['ADX'] = data['DX'].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "    # Drop intermediate columns\n",
        "    data.drop(['High-Low', 'High-PrevClose', 'Low-PrevClose', 'UpMove', 'DownMove', '+DM', '-DM', 'ATR', '+DM_Smoothed', '-DM_Smoothed', '+DI', '-DI', 'DX'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "def print_date_range(data_frame):\n",
        "  first_row = data_frame.head(1)\n",
        "  last_row = data_frame.tail(1)\n",
        "\n",
        "  # Print or use the first and last rows\n",
        "  print(\"First Row:\")\n",
        "  print(first_row)\n",
        "\n",
        "  print(\"\\nLast Row:\")\n",
        "  print(last_row)\n",
        "\n",
        "processed_data = pre_process(stock_data)\n",
        "feature_functions = [esa, calculate_stochastic_oscillator, calculate_rsi, CCI, accumulate_distribution, cal_fsl_udl, calculate_ichimoku_cloud, calculate_adx]\n",
        "processed_data = calculate_features(processed_data, feature_functions)\n",
        "#calculate_features(processed_data)\n",
        "print(\"new features\")\n",
        "processed_data  = processed_data.dropna()\n",
        "\n",
        "#getting data to train from 1982-12-06 till 2023-11-29\n",
        "X_selected = processed_data.iloc[10490:]\n",
        "testing_data = processed_data.iloc[9000:9500]\n",
        "training_data = processed_data.iloc[8000:8800]\n",
        "\n",
        "print(print_date_range(training_data ))\n",
        "print(print_date_range(testing_data ))\n",
        "print(print_date_range(X_selected))\n",
        "\n",
        "columns_to_plot = ['rsi', 'ADX', 'CCI', 'A/D']\n",
        "\n",
        "testing_data_set = processed_data.iloc[10000:10100]\n",
        "\n",
        "# Plot the selected columns\n",
        "plt.figure(figsize=(10, 6))\n",
        "for column in columns_to_plot:\n",
        "    plt.plot(X_selected.index, X_selected[column], label=column)\n",
        "\n",
        "# Add labels and legend\n",
        "plt.title('Plot of Selected Columns')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Column Values')\n",
        "plt.legend()\n",
        "print(\"X_SELECTED LENGTH\" , len(X_selected))\n",
        "# Show the plot\n",
        "plt.show()\n",
        "print(X_selected.head())\n",
        "print(processed_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s1lqW25YzMFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyJvkQKu5eWw"
      },
      "outputs": [],
      "source": [
        "#Implementing parallelsims\n",
        "class Parallel():\n",
        "  # Evaluate fitness in parallel\n",
        "\n",
        "  def parallel_evaluate_fitness(self, subpopulation, fitness_function):\n",
        "    return [fitness_function(individual) for individual in subpopulation]\n",
        "\n",
        "  def parallel_apply_genetic_operators(self,subpopulation, crossover_function, mutation_function):\n",
        "    new_generation = []\n",
        "    for i in range(0, len(subpopulation), 2):\n",
        "        parent1 = subpopulation[i]\n",
        "        parent2 = subpopulation[i + 1] if i + 1 < len(subpopulation) else subpopulation[i]\n",
        "        child1 = crossover_function(parent1, parent2)\n",
        "        child2 = crossover_function(parent2, parent1)\n",
        "\n",
        "\n",
        "\n",
        "        child1 = mutation_function(child1)\n",
        "        child2 = mutation_function(child2)\n",
        "\n",
        "        new_generation.extend([child1, child2])\n",
        "\n",
        "    return new_generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u08fvtE2w7SO"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from sklearn.model_selection import train_test_split\n",
        "except:\n",
        "  !pip install -U scikit-learn\n",
        "  from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "import multiprocessing\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Filter out the specific UserWarning from scikit-learn\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn.base\")\n",
        "\n",
        "\n",
        "\n",
        "class Chromosome():\n",
        "  def __init__(self, num_features, gene):\n",
        "      #np array\n",
        "      self.days = ''.join(random.choice('01') for _ in range(7))\n",
        "      #gene + self.days\n",
        "      self.genes = gene\n",
        "\n",
        "\n",
        "\n",
        "class Population():\n",
        "  def __init__(self, population_size,dataset, testing_dataset, num_features =15 ):\n",
        "      #generates our initial population -> rep of 1,0 for what feature to include and what not to\n",
        "      self.population = []\n",
        "      self.model = None\n",
        "      self.features = dataset[[\"200day\", \"50day\", \"rsi\", \"%K\", \"%D\", \"CCI\", \"A/D\", \"udl\", \"fsl\", \"TR\", \"ADX\", \"Tenkan-sen\", \"Kijun-sen\", \"Senkou Span A\", \"Senkou Span B\", \"close\"]]\n",
        "      self.testing = testing_dataset\n",
        "      self.target_vals = dataset[\"Target\"]\n",
        "\n",
        "      self.max_fitness = 0\n",
        "      self.num_features = num_features\n",
        "      self.best_solution = None\n",
        "      self.population_size = population_size\n",
        "      self.gene_length = num_features\n",
        "      self.init_pop()\n",
        "      #self.feature_tracker = multiprocessing.Manager().dict()\n",
        "      self.feature_tracker = defaultdict(int)\n",
        "\n",
        "  #find number of days for look back in lstm\n",
        "  def decode(self, chromosome):\n",
        "        # Convert binary to integer\n",
        "        return int(self.genes[15:], 2)\n",
        "\n",
        "  #generate init population\n",
        "  def init_pop(self):\n",
        "      seed_counter = 0\n",
        "      for i in range(self.population_size):\n",
        "            # Use a unique seed for each chromosome\n",
        "            seed = int(time.time()) + seed_counter\n",
        "            np.random.seed(seed)\n",
        "\n",
        "            self.population.append(Chromosome(self.num_features, np.random.randint(2, size=self.num_features)))\n",
        "\n",
        "            # Increment the counter for the next chromosome\n",
        "            seed_counter += 1\n",
        "\n",
        "  def train_model(self, X_train, y_train, model):\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1)\n",
        "\n",
        "  def lstm_model(self, dataset, chromosome, time_steps=20):\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    time_steps = 28\n",
        "\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(scaled_data) - time_steps):\n",
        "      sequences.append(scaled_data[i: i + time_steps, :])\n",
        "      labels.append(self.target_vals[i+time_steps])\n",
        "\n",
        "    X, y = np.array(sequences), np.array(labels)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(units=50, return_sequences=True))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    #loss = model.evaluate(X_test, y_test)\n",
        "    self.train_model(X_train, y_train, model)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (predictions >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(y_test, binary_predictions)\n",
        "    conf_matrix = confusion_matrix(y_test, binary_predictions)\n",
        "    if accuracy > self.max_fitness:\n",
        "      print(\"surpassed accuracy -----------\")\n",
        "      #update counter\n",
        "      self.model = model\n",
        "      print(\"MY MODEL\", self.model)\n",
        "      print(\"MY CHROMOSE\",  chromosome)\n",
        "      #elf.tracker_features(chromosome)\n",
        "    #print('Confusion Matrix:')\n",
        "    #print(conf_matrix)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    #print(\"my LSTM accuracy\", accuracy)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def random_forest_fitness(self, chromosome):\n",
        "        X, y = self.features, self.target_vals\n",
        "        y = y.iloc[300:]\n",
        "\n",
        "        # Select features based on the chromosome\n",
        "        selected_features = [i for i in range(self.num_features) if chromosome.genes[i] == 1]\n",
        "        if not selected_features:\n",
        "            return 0\n",
        "        X_selected = X.iloc[300:, selected_features]\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=None)\n",
        "        #print(\"MY TRAINING DATA\", X_train.values)\n",
        "        # Train a model and evaluate accuracy\n",
        "        model = RandomForestClassifier()\n",
        "        model.fit(X_train.values, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        return accuracy\n",
        "\n",
        "  def evaluate_fitness(self, chromosome):\n",
        "        # Create a simple dataset for demonstration purposes\n",
        "        #chromosome_feature_indices = chromosome.genes[:15]\n",
        "        indices_of_ones = np.where(chromosome.genes == 1)[0]\n",
        "        #lookback_days = self.decode(chromosome)\n",
        "        # Names of columns selected by the chromosome\n",
        "        selected_columns = self.features.columns[indices_of_ones]\n",
        "\n",
        "        if not selected_columns.all():\n",
        "            return 0\n",
        "        data_selected = self.features.loc[:, selected_columns]\n",
        "\n",
        "        acc = self.lstm_model(data_selected, chromosome)\n",
        "        #acc = self.random_forest_fitness(chromosome)\n",
        "        return acc\n",
        "\n",
        "  #def constraint_satisfaction(self):\n",
        "\n",
        "  def lookup_indices(self, search_list):\n",
        "    #time complexit of O(N + M)\n",
        "    column_names = [\"200day\", \"50day\", \"rsi\", \"%K\", \"%D\", \"CCI\", \"A/D\", \"udl\", \"fsl\", \"TR\", \"ADX\", \"Tenkan-sen\", \"Kijun-sen\", \"Senkou Span A\", \"Senkou Span B\", \"close\"]\n",
        "    column_indices = {column_name: index for index, column_name in enumerate(column_names)}\n",
        "    # Use the dictionary to find indices efficiently\n",
        "    indices = [column_indices[item] for item in search_list if item in column_indices]\n",
        "    return indices\n",
        "\n",
        "  #indication of downward movements\n",
        "  def trading_rules(self, selected_columns, row_index):\n",
        "    rules_down = {\n",
        "    'udl': lambda x:  x< 0 ,\n",
        "    'CCI': lambda x: x > 100,\n",
        "    'rsi': lambda x: x > 70,\n",
        "    'fsl': lambda x: x > 80,\n",
        "    #suggests the prescence of a significant trend\n",
        "    'ADX': lambda x: x > 25,\n",
        "    '50day': lambda row_index, df: df.iloc[row_index, df.columns.get_loc(\"50day\")] > df.iloc[row_index, df.columns.get_loc(\"close\")],\n",
        "    '200day': lambda row_index, df: df.iloc[row_index,  df.columns.get_loc(\"200day\")] > df.iloc[row_index, df.columns.get_loc(\"close\")],\n",
        "  }\n",
        "\n",
        "    df = self.features\n",
        "    down_price_features, up_price_features = [], []\n",
        "    #count number of true vs false\n",
        "    #true indicates prices going down and false indicates price going up\n",
        "    for column_name in selected_columns:\n",
        "      rule_function = rules_down.get(column_name)\n",
        "      if rule_function is not None:\n",
        "          if column_name in ['50day', '200day']:\n",
        "            result = rule_function(row_index, df)\n",
        "          else:\n",
        "            result = rule_function(df.iloc[row_index, df.columns.get_loc(column_name)])\n",
        "            #result = df.at[row_index, column_name].apply(rule_function)\n",
        "          #print(f\"Results for {column_name}: {result}\")\n",
        "          # we will change our mutation prob based on the prescence value of adx\n",
        "          if 'ADX' == column_name and result.all():\n",
        "            down_price_features.append(column_name)\n",
        "            up_price_features.append(column_name)\n",
        "          else:\n",
        "            down_price_features.append(column_name) if result.all() else up_price_features.append(column_name)\n",
        "    best_features = down_price_features if len(down_price_features) > len(up_price_features) else up_price_features\n",
        "    return self.lookup_indices(best_features)\n",
        "    #print(f'my final price movement showcase ', best_features)\n",
        "\n",
        "\n",
        "  def analyse_chromosome_mutation(self, individual,stock_data_row, random_number):\n",
        "      #go through each of the features\n",
        "      #individual = [1, 0 ,1 ,1, 0, 1 ,1, 1 ,0 ,1 ,1, 0, 0, 1 ,0]\n",
        "      #chromosome = np.array(individual)\n",
        "      #get top 3 indices and just dont index those\n",
        "      indices_of_ones = np.where(individual.genes == 1)[0]\n",
        "      #names of columns selected by chromosome\n",
        "      selected_columns = self.features.columns[indices_of_ones]\n",
        "      return self.trading_rules(selected_columns=selected_columns, row_index=random_number)\n",
        "\n",
        "\n",
        "\n",
        "  def mutation(self, individual, mutation_prob = 0.2, start_range=0):\n",
        "\n",
        "        #random_number = random.randint(6000, 10000)\n",
        "        random_number = random.randint(0, 124)\n",
        "        #pick a random day in our stock dataset between 2021->2023 year\n",
        "        rsi_sma_psi = processed_data.iloc[random_number]\n",
        "        best_feature_list = self.analyse_chromosome_mutation(individual,rsi_sma_psi, random_number)\n",
        "\n",
        "        #mutation_prob = 0.1 if 10 in best_feature_list else 0.3\n",
        "        #and i not in best_feature_lis\n",
        "        print(\"my tracker after changes in mutation\", self.feature_tracker)\n",
        "        #and i != top_keys\n",
        "        top_keys = sorted(self.feature_tracker, key=self.feature_tracker.get, reverse=True)[:3]\n",
        "        for i in range(start_range,self.gene_length):\n",
        "            if random.random() < mutation_prob and i not in top_keys:\n",
        "                individual.genes[i] = 1 - individual.genes[i]\n",
        "        return individual\n",
        "\n",
        "  def chromosome(self, parent1, parent2):\n",
        "    crossover_point = random.randint(1,  self.gene_length  - 2)\n",
        "    #check if rsi and msad show the same direction for stock price prediction then keep them in the final solution\n",
        "    child_chromosome = np.concatenate((parent1.genes[:crossover_point], parent2.genes[crossover_point:]))\n",
        "    new_child = Chromosome(15, child_chromosome)\n",
        "    return new_child\n",
        "\n",
        "  def tracker_features(self, best_chromosome):\n",
        "      for i in range(self.gene_length):\n",
        "        if best_chromosome.genes[i] == 1:\n",
        "          self.feature_tracker[i] += 1\n",
        "\n",
        "      print(\"my tracker after changes\", self.feature_tracker)\n",
        "\n",
        "  # Main function for parallel genetic algorithm\n",
        "  def parallel_genetic_algorithm(self, population_size, max_generations, num_processes):\n",
        "      pool = multiprocessing.Pool(processes=num_processes)\n",
        "      losses = []\n",
        "      # Initialize population\n",
        "      #population = initialize_population(population_size, individual_size)\n",
        "      parallel_compute = Parallel()\n",
        "\n",
        "      for generation in range(max_generations):\n",
        "          print(\"MY GENERATION----------\", generation)\n",
        "\n",
        "          # Divide population into subpopulations\n",
        "          subpopulations = [self.population[i:i + population_size // num_processes] for i in range(0, population_size, population_size // num_processes)]\n",
        "\n",
        "          # Parallel fitness evaluation\n",
        "          fitness_results = pool.starmap(parallel_compute.parallel_evaluate_fitness, [(subpop, self.evaluate_fitness) for subpop in subpopulations])\n",
        "          print(\"TRACKER\", self.feature_tracker)\n",
        "          # Flatten fitness results\n",
        "          fitness_values = [fit for sublist in fitness_results for fit in sublist]\n",
        "\n",
        "          # Select individuals for the next generation (simple truncation selection)\n",
        "          selected_indices = sorted(range(len(fitness_values)), key=lambda k: fitness_values[k], reverse=True)[:population_size]\n",
        "            # Extract fitness values of selected individuals\n",
        "          selected_fitness_values = [fitness_values[i] for i in selected_indices]\n",
        "\n",
        "            # Find the maximum fitness value among the selected individuals\n",
        "          max_fitness_value = max(selected_fitness_values)\n",
        "\n",
        "          print(\"MY MAX FITNESS VALUE\", max_fitness_value)\n",
        "\n",
        "          max_fitness_chromosomes = [self.population[i] for i in selected_indices if fitness_values[i] == max_fitness_value]\n",
        "          print(\"CHROMSOME FOR MAX VALUE -------------\", max_fitness_chromosomes[0].genes )\n",
        "          losses.append(max_fitness_value)\n",
        "\n",
        "          # Create the next generation\n",
        "          next_generation = [self.population[i] for i in selected_indices]\n",
        "\n",
        "          # Parallel genetic operations (crossover and mutation)\n",
        "          next_generation = pool.starmap(parallel_compute.parallel_apply_genetic_operators, [(subpop, self.chromosome, self.mutation) for subpop in subpopulations])\n",
        "\n",
        "          # Flatten the next generation\n",
        "          self.population = [ind for sublist in next_generation for ind in sublist]\n",
        "\n",
        "      pool.close()\n",
        "      pool.join()\n",
        "\n",
        "      # Obtain final solution(s)\n",
        "      final_solution = max(self.population, key=self.evaluate_fitness)\n",
        "\n",
        "      return final_solution\n",
        "\n",
        "  def tournament_selection(self):\n",
        "        # Randomly select individuals for the tournament\n",
        "        tournament_candidates = random.sample(self.population, self.tournament_size)\n",
        "\n",
        "        # Evaluate fitness for each candidate in the tournament\n",
        "        tournament_fitness_values = [self.evaluate_fitness(candidate) for candidate in tournament_candidates]\n",
        "\n",
        "        # Find the index of the best-performing individual in the tournament\n",
        "        winner_index = tournament_candidates[tournament_fitness_values.index(max(tournament_fitness_values))]\n",
        "\n",
        "        return winner_index\n",
        "\n",
        "  def evolve(self, generations=50, elitisim_size = 10):\n",
        "    losses = []\n",
        "    for generation in tqdm(range(generations), desc=\"Generations\"):\n",
        "      fitness_scores = [self.evaluate_fitness(chrom) for chrom in self.population]\n",
        "      best_index = np.argmax(fitness_scores)\n",
        "      best_chromosome = self.population[best_index]\n",
        "      best_fitness_val = fitness_scores[best_index]\n",
        "      losses.append(best_fitness_val)\n",
        "      print(f\"generation {generation}\")\n",
        "      print(f\"best_index {best_chromosome.genes}\")\n",
        "      print(f\"best_fitness_value {best_fitness_val}\")\n",
        "      if best_fitness_val > self.max_fitness:\n",
        "        #update crossover to use what features gave the best fitness and keep those binary digists fixed anad change other parts or do this for mutation as well\n",
        "        self.max_fitness = best_fitness_val\n",
        "        self.best_solution = best_chromosome.genes\n",
        "        print(f\"Generation {generation+1}: Best Fitness = {best_fitness_val}\")\n",
        "\n",
        "      elite_population = sorted(self.population, key=lambda x: self.evaluate_fitness(x), reverse=True)[:elitisim_size]\n",
        "\n",
        "            # Genetic operations: Crossover and Mutation\n",
        "      new_population = elite_population.copy()\n",
        "      for _ in range((self.population_size - elitisim_size) // 2):\n",
        "\n",
        "                parent1, parent2 = random.sample(elite_population, 2)\n",
        "                child1 = self.chromosome(parent1, parent2)\n",
        "                child2 = self.chromosome(parent2, parent1)\n",
        "\n",
        "                self.mutation(child1)\n",
        "                self.mutation(child2)\n",
        "\n",
        "                new_population.extend([child1, child2])\n",
        "\n",
        "      self.population = new_population\n",
        "    print(f'Our best lowest error value with the chromosome that gave it', self.max_fitness, self.best_solution)\n",
        "    indices_of_ones = np.where(self.best_solution == 1)[0]\n",
        "    return losses\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pop = Population(population_size=10, num_features=15, dataset=training_data, testing_dataset=testing_data_set)\n",
        "    #losses = pop.evolve(generations=5)\n",
        "    losses = pop.parallel_genetic_algorithm(30, 20, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EBO0I7HZSf_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "print (pop.model)\n",
        "def build_model(dataset, test, d_target, x_test):\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    time_steps = 28\n",
        "\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(scaled_data) - time_steps):\n",
        "      sequences.append(scaled_data[i: i + time_steps, :])\n",
        "      labels.append(d_target[i+time_steps])\n",
        "\n",
        "    X, y = np.array(sequences), np.array(labels)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
        "\n",
        "    #model = Sequential()\n",
        "    #model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    #model.add(LSTM(units=50, return_sequences=True))\n",
        "    #model.add(LSTM(units=50))\n",
        "   # model.add(Dense(units=1))\n",
        "\n",
        "    #model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    #model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "    #pop.model\n",
        "    predictions = pop.model.predict(test)\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "    threshold = 0.5\n",
        "    binary_predictions = (predictions >= threshold).astype(int)\n",
        "    accuracy = accuracy_score(x_test[:480], binary_predictions[:480])\n",
        "    print(\"accuracy\", accuracy)\n",
        "    return binary_predictions\n",
        "    conf_matrix = confusion_matrix(y_test, binary_predictions)\n",
        "\n",
        "def test_stock(dataset):\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "    scaled_data = scaler.fit_transform(dataset)\n",
        "    time_steps = 20\n",
        "\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(scaled_data) - time_steps):\n",
        "      sequences.append(scaled_data[i: i + time_steps, :])\n",
        "    seq = np.array(sequences)\n",
        "    print(seq.shape)\n",
        "    return np.array(sequences)\n",
        "\n",
        "def generate_testing():\n",
        "    sol = pop.best_solution\n",
        "    indices_of_ones = np.where(sol == 1)[0]\n",
        "\n",
        "    data = X_selected[[\"200day\", \"50day\", \"rsi\", \"%K\", \"%D\", \"CCI\", \"A/D\", \"udl\", \"fsl\", \"TR\", \"ADX\", \"Tenkan-sen\", \"Kijun-sen\", \"Senkou Span A\", \"Senkou Span B\", \"close\"]]\n",
        "    print(data.columns[indices_of_ones])\n",
        "    d_target = X_selected[\"Target\"]\n",
        "    print(data.columns)\n",
        "    selected_columns = data.columns[indices_of_ones]\n",
        "    data_selected = X_selected[['200day', \"rsi\",'%D',\"fsl\", 'A/D', 'TR', 'Tenkan-sen', 'Kijun-sen']]\n",
        "\n",
        "    X = test_stock(testing_data[['200day',\"rsi\", '%D',\"fsl\", 'A/D', 'TR', 'Tenkan-sen', 'Kijun-sen']])\n",
        "    X_test = testing_data[\"Target\"]\n",
        "    print(X.shape)\n",
        "    binary_predictions = build_model(data_selected, X, d_target, X_test)\n",
        "\n",
        "    actual_val = X_test\n",
        "    index_values = testing_data.index\n",
        "\n",
        "    # Line plot for actual values\n",
        "    plt.plot(index_values[:80], actual_val[:80], label=\"Actual Values\", marker='o',linestyle='None', color='blue')\n",
        "\n",
        "\n",
        "    #conf_matrix = sklearn.metrics.confusion_matrix(actual_val, binary_predictions)\n",
        "    # Line plot for predicted values\n",
        "    plt.plot(index_values[:80], binary_predictions[:80], label=\"Predicted Values\", linestyle='None',  marker='^', color='orange')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Values (0 if down or 1 if up)\")\n",
        "    plt.title(\"Actual vs Predicted Values\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "generate_testing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKFTHlDF7iVt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZPwGrMiw_bU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "plt.plot(range(1, len(losses) + 1), losses)\n",
        "plt.title('Genetic Algorithm -LSTM Accuracy Curve')\n",
        "plt.xlabel('Generation')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}